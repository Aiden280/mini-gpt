// GPTDecoder Model Structure
digraph {
	A [label=GPTDecoder]
	B [label="embedding: Embedding"]
	A -> B
	C [label="layers: ModuleList"]
	A -> C
	D [label="layers.0: GPTDecoderLayer"]
	C -> D
	DSA [label="layers.0.self_attention: MultiHeadAttention"]
	D -> DSA
	DSAWQ [label="layers.0.self_attention.w_qs: Linear"]
	DSA -> DSAWQ
	DSAWK [label="layers.0.self_attention.w_ks: Linear"]
	DSA -> DSAWK
	DSAWV [label="layers.0.self_attention.w_vs: Linear"]
	DSA -> DSAWV
	DSAFC [label="layers.0.self_attention.fc: Linear"]
	DSA -> DSAFC
	DSAA [label="layers.0.self_attention.attention: ScaledDotProductAttention"]
	DSA -> DSAA
	DSADO [label="layers.0.self_attention.attention.dropout: Dropout"]
	DSAA -> DSADO
	DSASM [label="layers.0.self_attention.attention.softmax: Softmax"]
	DSAA -> DSASM
	DSAD [label="layers.0.self_attention.dropout: Dropout"]
	DSA -> DSAD
	DSALN [label="layers.0.self_attention.layer_norm: LayerNorm"]
	DSA -> DSALN
	DFF [label="layers.0.feed_forward: PositionwiseFeedForward"]
	D -> DFF
	DFFS [label="layers.0.feed_forward.fc: Sequential"]
	DFF -> DFFS
	DFFSL1 [label="layers.0.feed_forward.fc.0: Linear"]
	DFFS -> DFFSL1
	DFFSG [label="layers.0.feed_forward.fc.1: GELU"]
	DFFS -> DFFSG
	DFFSD [label="layers.0.feed_forward.fc.2: Dropout"]
	DFFS -> DFFSD
	DFFSL2 [label="layers.0.feed_forward.fc.3: Linear"]
	DFFS -> DFFSL2
	DFFSLN [label="layers.0.feed_forward.fc.4: LayerNorm"]
	DFFS -> DFFSLN
	DLN1 [label="layers.0.layer_norm1: LayerNorm"]
	D -> DLN1
	DLN2 [label="layers.0.layer_norm2: LayerNorm"]
	D -> DLN2
	DDO [label="layers.0.dropout: Dropout"]
	D -> DDO
	E [label="layers.1: GPTDecoderLayer"]
	C -> E
	ESA [label="layers.1.self_attention: MultiHeadAttention"]
	E -> ESA
	ESAWQ [label="layers.1.self_attention.w_qs: Linear"]
	ESA -> ESAWQ
	ESAWK [label="layers.1.self_attention.w_ks: Linear"]
	ESA -> ESAWK
	ESAWV [label="layers.1.self_attention.w_vs: Linear"]
	ESA -> ESAWV
	ESAFC [label="layers.1.self_attention.fc: Linear"]
	ESA -> ESAFC
	ESAA [label="layers.1.self_attention.attention: ScaledDotProductAttention"]
	ESA -> ESAA
	ESADO [label="layers.1.self_attention.attention.dropout: Dropout"]
	ESAA -> ESADO
	ESASM [label="layers.1.self_attention.attention.softmax: Softmax"]
	ESAA -> ESASM
	ESAD [label="layers.1.self_attention.dropout: Dropout"]
	ESA -> ESAD
	ESALN [label="layers.1.self_attention.layer_norm: LayerNorm"]
	ESA -> ESALN
	EFF [label="layers.1.feed_forward: PositionwiseFeedForward"]
	E -> EFF
	EFFS [label="layers.1.feed_forward.fc: Sequential"]
	EFF -> EFFS
	EFFSL1 [label="layers.1.feed_forward.fc.0: Linear"]
	EFFS -> EFFSL1
	EFFSG [label="layers.1.feed_forward.fc.1: GELU"]
	EFFS -> EFFSG
	EFFSD [label="layers.1.feed_forward.fc.2: Dropout"]
	EFFS -> EFFSD
	EFFSL2 [label="layers.1.feed_forward.fc.3: Linear"]
	EFFS -> EFFSL2
	EFFSLN [label="layers.1.feed_forward.fc.4: LayerNorm"]
	EFFS -> EFFSLN
	ELN1 [label="layers.1.layer_norm1: LayerNorm"]
	E -> ELN1
	ELN2 [label="layers.1.layer_norm2: LayerNorm"]
	E -> ELN2
	EDO [label="layers.1.dropout: Dropout"]
	E -> EDO
	F [label="layers.2: GPTDecoderLayer"]
	C -> F
	FSA [label="layers.2.self_attention: MultiHeadAttention"]
	F -> FSA
	FSAWQ [label="layers.2.self_attention.w_qs: Linear"]
	FSA -> FSAWQ
	FSAWK [label="layers.2.self_attention.w_ks: Linear"]
	FSA -> FSAWK
	FSAWV [label="layers.2.self_attention.w_vs: Linear"]
	FSA -> FSAWV
	FSAFC [label="layers.2.self_attention.fc: Linear"]
	FSA -> FSAFC
	FSAA [label="layers.2.self_attention.attention: ScaledDotProductAttention"]
	FSA -> FSAA
	FSADO [label="layers.2.self_attention.attention.dropout: Dropout"]
	FSAA -> FSADO
	FSASM [label="layers.2.self_attention.attention.softmax: Softmax"]
	FSAA -> FSASM
	FSAD [label="layers.2.self_attention.dropout: Dropout"]
	FSA -> FSAD
	FSALN [label="layers.2.self_attention.layer_norm: LayerNorm"]
	FSA -> FSALN
	FFF [label="layers.2.feed_forward: PositionwiseFeedForward"]
	F -> FFF
	FFFS [label="layers.2.feed_forward.fc: Sequential"]
	FFF -> FFFS
	FFFSL1 [label="layers.2.feed_forward.fc.0: Linear"]
	FFFS -> FFFSL1
	FFFSG [label="layers.2.feed_forward.fc.1: GELU"]
	FFFS -> FFFSG
	FFFSD [label="layers.2.feed_forward.fc.2: Dropout"]
	FFFS -> FFFSD
	FFFSL2 [label="layers.2.feed_forward.fc.3: Linear"]
	FFFS -> FFFSL2
	FFFSLN [label="layers.2.feed_forward.fc.4: LayerNorm"]
	FFFS -> FFFSLN
	FLN1 [label="layers.2.layer_norm1: LayerNorm"]
	F -> FLN1
	FLN2 [label="layers.2.layer_norm2: LayerNorm"]
	F -> FLN2
	FDO [label="layers.2.dropout: Dropout"]
	F -> FDO
	G [label="fc: Linear"]
	A -> G
}
